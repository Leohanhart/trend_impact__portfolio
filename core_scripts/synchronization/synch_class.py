# -*- coding: utf-8 -*-
"""
Created on Fri Jun  4 17:13:59 2021

@author: Gebruiker
"""
import cloudpickle as pickle
import os
import numpy as np
import pandas as pd
import stat
import time
import datetime
from datetime import datetime
# what is data is relaing on parrameters.
import constants


class data_synch:
    """

    Error's will be listed in self.error*+extenction

    Knowledge":
         this class is made for two things, loading pre calculated indicators, also with the purpose to 
         work beter with the data. Load data faster, save time durring reloads, analyses and so on.

         There are two options: 
             one:   Loading time serie data, if expirerd syncing the data, using the data, adding new data without past file.
             two:   Loading information who bearly changes or is now heavy. Like EPS data, company data, trade data. 


    How to work with this class: 


        ____Instructions update data timeseries_______

        WARNING: 
            this only works for time series. 

        Knowledge. 
        the best way to work with this kind of objects is to created it. Add add static data by "object.prepairstatic" this always needs to be a stockobject. 
        then loading the data. Check if its epired, if expired loading howmuch rows it is expired(this is depended on the static data)

        1. Make a synch object, with the file initals, including ticker and type of object ( for example "GOOG" and "EPS". 

        2. check if the data is new? 

        Yes, add the whole data set including, synch_data/save_data and return 

        No, add the static_data with the functing : prepair stock data for static data.
        get the amount of rows that needs to be 

        __instructions for updating les important data, like company data__


        1. Make a synch object, with the file initals, including ticker and type of object ( for example "GOOG" and "EPS". 

        2. after that, you always get the new data, and then you overwrite the data. 

        if you have your own tradeobjects or so on, you just load it. If its not empty create one. 


    """
    # stockdata

    # path of analyses
    path = os.path.abspath(os.getcwd())

    #
    # ticker of the stock
    ticker = ""

    # typ of data
    type_of_data = ""

    # type of extenction: liq_data = liquidity. skew_data = skew.
    data_extention = ""

    # subfolder
    subfolder = ""

    # data from loaded file
    retreived_data = ""

    # data from other class to check for validation
    static_data = ""

    # newdata, data that has been generated by a function:
    new_data = ""

    # product after the data's have merged.
    synchronized_data = ""

    # error code for if something goes wrong.
    error_code = 0

    # error return int
    error_type_int = 0

    # full path name.
    Full__filename = 0

    # New data
    is_data_new = False

    # print data
    print_data: bool = True

# functions that should be there.

# Load_data (path-subfolder-,-"ticker","Type_of_data")
# Save_data (path-subfolder-,-"ticker","Type_of_data")
# Synch_data - merging data.
# data_for_callculations

    def __init__(self, path: str = "", subfolder: str = "", ticker: str = "", data_extention: str = "", raise_error_if_found: bool = False):
        """
        Initalize data synchronizations

        Parameters
        ----------
        path : TYPE, optional
            DESCRIPTION. The default is "".
        subfolder : TYPE, optional
            DESCRIPTION. The default is "".
        ticker : TYPE, optional
            DESCRIPTION. The default is "".
        data_extention : TYPE, optional
            DESCRIPTION. The default is "".

        Returns
        -------
        int
            DESCRIPTION.

        """
        # checking if the path is emtpy, not add the system path.
        if path == "":
            # this option underhere was used in the past sessios, this is replaced with chagens to filefolder path instate of work directory
            #path = os.path.abspath(os.getcwd())
            newpath = __file__
            new_path_to_foler = '\\'.join(newpath.split('\\')[0:-1])
            path = new_path_to_foler

        # checking if the
        if subfolder == "" or ticker == "" or data_extention == "":
            self.error_code = "Data missing durring initalization"
            self.error_number = 404
            raise FileNotFoundError
            return 0

        # creating file path.
        self.file_name = ticker + "." + data_extention

        self.raise_error_if_found = raise_error_if_found
        # creating class object vars - this is done for refreshin the path extention.

        self.ticker = ticker
        self.path = path
        self.subfolder = subfolder

        # adding path to path

        self.path_with_subfolder = os.path.join(self.path, self.subfolder)
        full_file_path = os.path.join(self.path_with_subfolder, self.file_name)

        self.data_extention = data_extention

        # constructing the path file.
        # path +"\\" + subfolder +"\\" + ticker+ "."  + data_extention
        self.Full__filename = full_file_path

        # Loading the data - this step is very important.
        self.load_data()

        if 0 == 0:
            pass

    def refresh_object(self):
        """
        Option to rest ticker+extention, can be used for re using the object.

        Returns
        -------
        None.

        """
        #self.Full__filename = self.path +"\\" + self.subfolder +"\\" + self.ticker+ "."  + self.data_extention
        # adding path to path

        file_name = self.ticker + "." + self.data_extention

        full_file_path = os.path.join(self.path_with_subfolder, file_name)

        self.Full__filename = full_file_path

        return
        #self.path_with_subfolder = os.path.join(self.path,self.subfolder)
        #full_file_path = os.path.join(self.path_with_subfolder, file_name)

    def prepair_stock_data_for_static_data(self, data=""):

        if type(data) != str:
            self.static_data = pd.DataFrame(data[["Date", "Open"]])

    def prepair_data_in_dataframe(self, data, type_of_data="Array"):
        """
        Class used for packaging data in dataframe before the save. 







        Parameters
        ----------
        data : TYPE
            DESCRIPTION.
        type_of_data : TYPE, optional
            DESCRIPTION. The default is "Array".

        Returns
        -------
        None.

        """

        # checks if the data to get the dates from is loadeble.
        if type(self.static_data) != str:

            if self.print_data:

                print("Here should be the code.")
        # sets incomming data to the data.

        #print("\n\n\n Iam the greatone\n\n")

        #print(type(data), " This is the data type ")
        #print(len(data), "this is the length of the data\n\n",len(self.static_data),"this is the len of the static data")

        # print(type(self.static_data))

        # print(self.static_data.columns)

        idata = self.static_data["Date"].tail(len(data))
        ldata = data

        ldataf = pd.DataFrame(ldata)

        data_index = idata.index
        # print(data_index)

        #liq_data = pd.concat([data_index,ldataf.reindex(data_index)],axis = 1)

        # print(liq_data)
        # volgens mij is liq data niet toegevoegd aan een dataframe. Dat is dus jammer.

        #print(len(ldataf), len(data_index))

        #datadasframe = pd.DataFrame(index=datesindex, data=ldataf.values)
        dasframe = pd.DataFrame(index=data_index, data=ldataf.values)
        # print(dasframe)
        dasframe.columns = ["Data"]
        self.new_data = dasframe

    def load_data(self):
        """
        Loads the data from initisalized path

        Returns
        -------
        int
            DESCRIPTION.

        """
        try:
            with open(self.Full__filename, 'rb') as handle:

                self.retreived_data = pickle.load(handle)

                if self.print_data:
                    print("data ", self.data_extention, "is loaded")

        except Exception as e:

            if self.print_data:
                print(e)

            self.error_code = "No data retreived"
            self.is_data_new = True

            if self.raise_error_if_found:
                raise FileNotFoundError

            return 0

    def save_data(self):
        """
        Saves the data from the initalized path. 

        Returns
        -------
        int
            DESCRIPTION.

        """
        if self.is_data_new == True:

            try:
                with open(self.Full__filename, 'wb') as handle:

                    pickle.dump(self.new_data, handle,
                                protocol=pickle.DEFAULT_PROTOCOL)
                    print("Data saved", self.ticker)
            except:
                self.error_code = 1
        else:

            try:
                with open(self.Full__filename, 'wb') as handle:

                    pickle.dump(self.synchronized_data, handle,
                                protocol=pickle.DEFAULT_PROTOCOL)
                    print("New data is saved", self.ticker)
            except:
                self.error_code = 1

    def returns_last_date_retreived_data(self, return_type=0):
        """


        Parameters
        ----------
        return_type : TYPE, optional
            DESCRIPTION. The default is 0.

        Returns
        -------
        TYPE
            DESCRIPTION.

        """
        # if return_type == "date.object"
        if type(self.retreived_data) == str:
            self.error_code = "Retreived data is no dataframe or empty5"

        # sets retreived data to df1 var.
        df1 = self.retreived_data

        # returns the last date from the data

        last_date_df1 = df1.index[len(df1)-1]

        # if return_type is date object. Return date_object
        if return_type == "date.object":
            date = last_date_df1.date()
            return date

        x = last_date_df1.date()
        x = x.strftime("%Y-%m-%d")

        return x

    def returns_last_date_static_data(self, return_type=0):
        """


        Parameters
        ----------
        return_type : TYPE, optional
            DESCRIPTION. The default is 0.

        Returns
        -------
        TYPE
            DESCRIPTION.

        """
        # if return_type == "date.object"
        if type(self.static_data) == str:
            self.error_code = "No static data avalible"

        # sets retreived data to df1 var.
        df1 = self.static_data

        # returns the last date from the data
        last_date_df1 = df1.index[len(df1)-1]

        # if return_type is date object. Return date_object
        if return_type == "date.object":
            date = last_date_df1.date()
            return date

        x = last_date_df1.date()
        x = x.strftime("%Y-%m-%d")

        return x

    def return_amount_of_days(self):

        last_date_retreived_data = self.returns_last_date_retreived_data()
        last_date_static____data = self.returns_last_date_static_data()

        frame1 = self.static_data[last_date_retreived_data:last_date_static____data]
        if len(frame1) != 0:
            return len(frame1)

    def returns_true_if_expired_with_data(self, return_type=0):
        date_retreived_data = self.returns_last_date_retreived_data()
        date_static_data = self.returns_last_date_static_data()

        if date_retreived_data < date_static_data:
            return True

    def synch_data_timeserie(self):
        """

        This function uses retreived data ( Data that has been oploaded.)

        Returns
        -------
        None.

        """
        # checks if the file we retreved was empty() not
        #print("Data synch activeated.")
        if self.is_data_new == True:

            if type(self.new_data) != str:

                self.save_data()

            elif type(self.new_data) == str:
                error_code = "No new data found"
                return False

        elif type(self.new_data) != str:
            # checks if we can work with both data.
            if type(self.retreived_data) != str and type(self.new_data) != str:
                if type(self.retreived_data) != type(self.new_data):
                    self.error_code = "Types of data do not patch durring syncronisation"
                else:

                    # setting data to work with easy
                    df1 = self.retreived_data
                    df2 = self.new_data

                    #print("These are the dataframes 1. ")
                    # print(df1.tail(5))

                    # print(df2.head(5))

                    # print("\n\n\n\n\n")
                    # finding the last one of the saved data.
                    last_date = df1.index[len(df1)-1]
                    x = last_date.date()
                    x = x.strftime("%Y-%m-%d")

                    # step 2. find the index of the new frame.
                    index = df2.index

                    # print(x)

                    # finds the index of where the retreived data went.
                    end_location_old_data = np.where(index >= x)[0].min()
                    # print(end_location_old_data)
                    # finds the last new data point is been.
                    end_location_new_data = np.where(index >= x)[0].max()
                    # print(end_location_new_data)
                    # creats the end section of the new frame. # plus one removed
                    additional_frame = df2.iloc[end_location_old_data +
                                                0: end_location_new_data]

                    # find last date
                    frames = [df1, additional_frame]

                    result = pd.concat(frames)
                    #print(len(result) , "this is the total len")
                    result = result.copy()
                    self.synchronized_data = result
                    self.save_data()
        else:
            #print("Not done")
            pass
            raise Exception("Note done")

    def overwrite_data(self):
        self.is_data_new = True
        self.save_data()

    def return_last_modification_file(self, difference: bool = True, view: str = "W", experiation_bool: bool = False):
        """
        this function returns the amount of days/weeks the data has been last modified. 

        Options for view / timeframe = 'D' is for day and 'W' for week.  

        if experation bool is truned true, it returns true if difference of time frame is more than 0. 




        Parameters
        ----------

        return_type : type, optional

            DESCRIPTION. The default is int.

        difference : bool, optional

            DESCRIPTION. The default is True.

        view : str, optional

            DESCRIPTION. The default is W "W for week or /D for day".

        experiation_bool : bool, optional

            DESCRIPTION. The default is False.

                        If true, returns boolean if date is expired. 

        Returns
        -------
        TYPE
            int of bool 

        """
        if self.is_data_new:

            raise Exception("Data is new. No data possible")

        else:

            # creates path
            modTimesinceEpoc = os.path.getmtime(self.Full__filename)
            # extracts the time frome path
            modificationTime = datetime.fromtimestamp(
                modTimesinceEpoc).strftime('%Y-%m-%d')
            # returns dateobject of path
            date_object_lastM = datetime.strptime(
                modificationTime, "%Y-%m-%d").date()

            # print(date_object_lastM)

            # get current date
            dateTimeObj = datetime.now().date()

            # returns the day betweem
            a = days_beween = dateTimeObj - date_object_lastM

            # converts the day between in int
            days_difference = a.days

            # returns amount of days.
            if difference and view == "D" and not experiation_bool:
                return days_difference

            # extract week nr's
            weekNr_current = dateTimeObj.isocalendar()[1]
            weekNr_old = date_object_lastM.isocalendar()[1]

            #print( weekNr_old,  " = Old weeks")

            amount_weeks_different = weekNr_current - weekNr_old

            if difference == True and view == "W" and not experiation_bool:
                #   print(amount_weeks_different, "= test")

                return amount_weeks_different

            if experiation_bool:
                if view == "D":
                    if days_difference != 0:
                        return True
                    else:
                        return False
                elif view == "W":
                    if amount_weeks_different != 0:
                        return True
                    else:
                        return False
                else:
                    raise Exception(
                        "Data is new. Wrong formate, Qould happend if D or W is not used.")

    @staticmethod
    def time_bewteen_time_series(indicator_time_serie="", data_time_serie="", type_of_periode="", return_bool_true_if_expired: bool = False):
        """
        function callculates the missed amount of calculations. 

        Why is this function build. To make sure no sink problem occures in the weekends. 

        This function is unit tested in fix_amount_refresh.py

        Parameters
        ----------
        indicator_time_serie : pandas.core.frame.DataFrame, required

        DESCRIPTION. this should be the -- x = analyses.analyeses_dictionary['indicator_timeserie_raw']
            type
        data_time_serie : pandas.core.frame.DataFrame, required

        DESCRIPTION. this should be the weekly_data/weekly data from the incomming stock_object

        type_of_periode : STR , required
            DESCRIPTION. The default is "", should be D or W 

        return_bool_true_if_expired : BOOL, optional

        DESCRIPTION. If turned on with = true, returns true if expired and fals if not. 

        Returns
        -------

        Integere of amount of iterations. 

        """
        # check if formates match

        if(
                not

                isinstance(indicator_time_serie, pd.core.frame.DataFrame)

                or

                not isinstance(data_time_serie, pd.core.frame.DataFrame)

        ):

            raise Exception("one of the timeseries is not the right formate")

        #
        # set indidcator timeserie
        x = indicator_time_serie

        #
        y = data_time_serie

        diff = len(y) - len(x)

        return diff

    @staticmethod
    def check_incomming_dict_legit(IncommingObject):
        """
        Check if object is legid.

        Parameters
        ----------
        IncommingObject : TYPE
            DESCRIPTION.

        Returns
        -------
        None.

        """
        if type(IncommingObject) != dict:
            return False

        return True


# remove comments to see how it works
"""
import stock_object_refactord
# test the system 
stock = stock_object_refactord.stockobject_rf("AAL")
data = stock.Stock_Data.tail(1000)


data_1 = data.head(600)
data_2 = data.tail(600)

data_sych = data_synch(path="", subfolder="Data_folder",ticker = "Test_3",data_extention="LOVE")

print( "\n\nPrint Error code = ",data_sych.error_code)
print( "\n\nPrint bool, new data", data_sych.is_data_new)

data_sych.prepair_stock_data_for_static_data(data)

data_sych.new_data = data_1
data_sych.overwrite_data()

print("New Session")
data_sych = data_synch(path="", subfolder="Data_folder",ticker = "Test_3",data_extention="LOVE")



print( "\n\nPrint Error code = ",data_sych.error_code)
print( "\n\nPrint bool, new data", data_sych.is_data_new)

print(data_synch.retreived_data)


data_sych.prepair_stock_data_for_static_data(data)
if not data_sych.new_data:
    data_sych.prepair_stock_data_for_static_data(data)
    
if data_sych.returns_true_if_expired_with_data():
    
    amount_rows_to_return = data_sych.return_amount_of_days()
    print("There where rows expired = ", amount_rows_to_return)
    data_sych.new_data = data_2
    
    data_sych.synch_data_timeserie()
    print(data_sych.error_code , "= error code")
    
    
print("\n\n")

data_sych = data_synch(path="", subfolder="Data_folder",ticker = "Test_3",data_extention="LOVE")

print(len(data_sych.retreived_data), "This is the new amount of rows. Should be 1000")
print(len(data))
""


# er moet een functie komen die de gewonnen data opslaat in een dataframe met data object. 
import stock_object_refactord
import stockobject_analyses
import pandas as pd
stock = stock_object_refactord.stockobject_rf("IRCP")
data = stock.Stock_Data.tail(2000)

liq_analyses = stockobject_analyses.liquidity_analyses(data)
data_liq = liq_analyses.liquidity_timeserie(60)
print(data_liq, "this is it")

global ldata
ldata = data_liq

ldataf = pd.DataFrame(ldata)
print(ldataf)

global idata
idata= data["Date"].tail(60)
print(idata)
data_index = idata.index
print(data_index)
global liq_data


#liq_data = pd.concat([data_index,ldataf.reindex(data_index)],axis = 1)

#print(liq_data)
# volgens mij is liq data niet toegevoegd aan een dataframe. Dat is dus jammer. 

print(len(ldataf), len(data_index))


#datadasframe = pd.DataFrame(index=datesindex, data=ldataf.values)
dasframe = pd.DataFrame(index=data_index, data=ldataf.values)
dasframe.columns = ["Data"]
print(dasframe)


print(liq_analyses.return_liquidty_analyses_last_Trading_day())
"""

# test dat het werkt zien.

"""
import stock_object_refactord
import stockobject_analyses
import pandas as pd
stock = stock_object_refactord.stockobject_rf("IRCP")
data = stock.Stock_Data.tail(2000)

liq_analyses = stockobject_analyses.liquidity_analyses(data)
data_liq = liq_analyses.liquidity_timeserie(10)
print(data_liq, "this is it")

idata= data["Date"].tail(len(data_liq))
ldata = data_liq

print(len(idata))
ldataf = pd.DataFrame(ldata)
print(ldataf, "Thisis the L data")
print(idata, "This is the I data")


print("\n\n",idata,"\n\n")
data_index = idata.index
print(data_index)
global liq_data


#liq_data = pd.concat([data_index,ldataf.reindex(data_index)],axis = 1)

#print(liq_data)
# volgens mij is liq data niet toegevoegd aan een dataframe. Dat is dus jammer. 

print(len(ldataf), len(data_index))


#datadasframe = pd.DataFrame(index=datesindex, data=ldataf.values)
dasframe = pd.DataFrame(index=data_index, data=ldataf.values)
dasframe.columns = ["Data"]

print(dasframe)



data_sych = data_synch(path="", subfolder="Data_folder",ticker = "Test_3",data_extention="LOVE")
data_sych.static_data = data
x= data_sych.prepair_data_in_dataframe(data_liq)
print(x, "This is x")

"""

if __name__ == "__main__":

    try:

        global synch_object
        try:

            synch_object = data_synch(path=constants.CORE_DATA_____PATH, subfolder="stock_analyses_data",
                                      ticker="SWN", data_extention="WMONEYFLOWS_dictonary")

        except Exception as e:

            print(e)

        # power_stock_apple.twitter_module(full_test_function=True)

    except Exception as e:

        raise Exception("Problem with the stockticker", e)
